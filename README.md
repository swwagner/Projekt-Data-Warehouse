# A: Discuss the purpose of this database in context of the startup, Sparkify, and their analytical goals.
A music streaming startup called Sparkify wants to analyze the data they have been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.
The data are stored in 2 public S3 buckets ( 1 for artists and songs and 1 for the user activities) and will be stored in an AWS Redshift Cluster.

# B: State and justify your database schema design and ETL pipeline.
## Source
There are two sources of data, both are in JSON format and consist of several files: 
1. The first dataset is a subset of real data from the Million Song Dataset. Each file contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
2. The second dataset consists of log files generated by this event simulator based on the songs in the dataset of source 1. These simulate activity logs from a music streaming app based on specified configurations. The log files in the dataset are partitioned by year and month. 
## Database schema design
In the project we load these 2 data sources into a cloud database, so we can fulfill the requirement of running analytical queries. Caused on the fact, that we want to run the queries very easily it is recommended to use the star schema in the database. The star schema is the simplest style of data mart schema and consists of one or more fact tables referring any number of dimension tables.
In our project we have 1 fact tables (songplays), which consists of the primary keys of each dimension table but also other attributes. There are 4 dimension tables (users, songs, artists, time), each of them consists of the primary key, which links to the fact table, and the respective attribute describing the dimension.
## ETL pipeline
There are used 4 different files to create this project (1 notebook documents (.ipynb), 1 condif script and 3 python scripts (.py)):
1. **execute.ipynd** is the python script to call following 3 python scripts and start the etl: etl.py, create_tables.py, sql_queries.py
2. **etl.py** is a python script to load the data from song_data and log_data into the star schema database. 
3. **create_tables.py** is a python script to create the database, create new tables and drop tables.
4. **sql_queries.py** is a python script that consists of all functions and sql-querys of the project including the 4 dimension tables.
4. **dwh.cfg** is a config script that holds all information for the AWS cluster and bucket.


